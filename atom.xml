<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Attractor Software Blog</title>
  
  
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="https://attractor-software.github.io/"/>
  <updated>2018-03-30T06:22:54.102Z</updated>
  <id>https://attractor-software.github.io/</id>
  
  <author>
    <name>Attractor Software LLC</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Digits Recognizer using Python and React. Train the model.</title>
    <link href="https://attractor-software.github.io/2018/03/25/digits-recognizer-1/"/>
    <id>https://attractor-software.github.io/2018/03/25/digits-recognizer-1/</id>
    <published>2018-03-25T08:02:19.000Z</published>
    <updated>2018-03-30T06:22:54.102Z</updated>
    
    <content type="html"><![CDATA[<p>Today’s topic is like an introduction into the combination of computer vision and machine learning. All that I’ll do here is just fitting kNN model with the data of handwritten digits taken from the <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST Database</a> and also checking its accuracy. </p><h2 id="Set-up-environment"><a href="#Set-up-environment" class="headerlink" title="Set up environment"></a>Set up environment</h2><p>I’ll be using <a href="https://anaconda.org/anaconda/python" target="_blank" rel="noopener">Anaconda</a> as my environment so I’ll skip the step about installing the dependencies.</p><h2 id="Import-dependencies"><a href="#Import-dependencies" class="headerlink" title="Import dependencies"></a>Import dependencies</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br></pre></td></tr></table></figure><h2 id="Prepare-data"><a href="#Prepare-data" class="headerlink" title="Prepare data"></a>Prepare data</h2><h3 id="Load-data"><a href="#Load-data" class="headerlink" title="Load data"></a>Load data</h3><p>The dataset from <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST Database</a> is available in the datasets module of sklearn, so let’s start with loading the data.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digits = datasets.load_digits()</span><br></pre></td></tr></table></figure></p><h3 id="Split-the-data"><a href="#Split-the-data" class="headerlink" title="Split the data"></a>Split the data</h3><p>Now we need to have two different datasets: one for testing and the other for training our model.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(X_train, X_test, y_train, y_test) = train_test_split(</span><br><span class="line">    digits.data, digits.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">42</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h2 id="Fit-the-model"><a href="#Fit-the-model" class="headerlink" title="Fit the model"></a>Fit the model</h2><h3 id="Find-best-k"><a href="#Find-best-k" class="headerlink" title="Find best k"></a>Find best k</h3><p>We can’t take k out of our mind, so let’s train model and evaluate accuracy for different k.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ks = np.arange(<span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line">scores = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> ks:</span><br><span class="line">    model = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line">    score.mean()</span><br><span class="line">    scores.append(score.mean())</span><br><span class="line">plt.plot(scores, ks)</span><br><span class="line">plt.xlabel(<span class="string">'accuracy'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'k'</span>)</span><br></pre></td></tr></table></figure></p><p>As the output we can see such plot:</p><p>Looking at this chart we can understand that the best accuracy was reached when k was 3.<br>So from now, we’ll be using k=3 for our model.</p><h3 id="Evaluate-the-model-on-the-test-data"><a href="#Evaluate-the-model-on-the-test-data" class="headerlink" title="Evaluate the model on the test data"></a>Evaluate the model on the test data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">z = model.predict(X_test)</span><br></pre></td></tr></table></figure><p>Let’s now create a classification report to see the accuracy<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(classification_report(y_test, z))</span><br></pre></td></tr></table></figure></p><p>Amazing! We reached 99% accuracy!</p><h1 id="To-be-continued"><a href="#To-be-continued" class="headerlink" title="To be continued"></a>To be continued</h1><p>In the next article I’ll create a <a href="https://reactjs.org/" target="_blank" rel="noopener">React</a> Application to draw the digits in order to join it with the classifier into a web application.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Today’s topic is like an introduction into the combination of computer vision and machine learning. All that I’ll do here is just fitting
      
    
    </summary>
    
    
  </entry>
  
</feed>
